{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Documentation of the NB_IoT environment\n",
    "\n",
    "This notebook explains how to use the NB_IoT environment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Global Parameters\n",
    "\n",
    "There are 3 global parameters that must be set with ```set_global_parameters``` defined in ```parameters.py```:\n",
    "\n",
    "* ```N```: the number of UEs included in the state observation\n",
    "* ```H```: (horizon) the number of future subframes observed in the states\n",
    "* ```Nc```: the number of carriers handled by the system\n",
    "\n",
    "The following lines must be include bebore any other ```import``` statement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# determine global parameters before any other system import\n",
    "from parameters import set_global_parameters\n",
    "set_global_parameters(N = 4, H = 40, Nc = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are two ways to create an instance of the NBIoT system:\n",
    "1) with the auxiliary function ```create_system``` in ```test_utils.py```\n",
    "2) element-by-element\n",
    "\n",
    "Let's start with the first way"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## System Creation with ```create_system```\n",
    "\n",
    "The arguments of this function are:\n",
    "* ```rng```: an random generator created with ```default_rng``` from ```numpy.random```\n",
    "* ```conf```: a dictionary with the system configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import system creation function\n",
    "from test_utils import create_system\n",
    "\n",
    "# random number generator object\n",
    "from numpy.random import default_rng"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# simulator configuration\n",
    "conf = {\n",
    "    'animate_carrier': True, # to generate an animation of the carrier occupation\n",
    "    'ratio': 1, # ratio of uniform/beta traffic\n",
    "    'M': 1500, # number of UEs\n",
    "    'levels': [0,1,2], # simulated CE levels\n",
    "    'buffer_range': [100, 500], # range for the number of bits in the UE buffer\n",
    "    'reward_criteria': 'average_delay', # there are multiple criteria defined in perf_monitor.py\n",
    "    'statistics': True, # to store historical data for statistical evaluation\n",
    "    'animate_stats': False, # to generate an animation of the statsitics over time\n",
    "    'sc_adjustment': True, # to automatically adjust the number of subcarriers\n",
    "    'mcs_automatic': True # to autimatically select mcs and Nrep\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create random number generator\n",
    "rng = default_rng(seed = 5)\n",
    "\n",
    "# create system\n",
    "node, perf_monitor, population, carrier = create_system(rng, conf)\n",
    "\n",
    "# check global variables\n",
    "print(f'carriers = {node.n_carriers}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can run a step-by-step simulation of the system as follows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this auxiliary function generates a simple default action\n",
    "from test_utils import generate_reasonable_action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reset the system\n",
    "o, info = node.reset()\n",
    "\n",
    "# generate initial action\n",
    "action = generate_reasonable_action(rng, o)\n",
    "\n",
    "# simulation loop\n",
    "n = 0\n",
    "while node.time < 500:\n",
    "    n += 1\n",
    "    o, r, Done, info = node.step(action) \n",
    "    action = generate_reasonable_action(rng, o)\n",
    "    if n % 10 == 0:\n",
    "        print(f'event: {n}, time {node.time}, reward: {r}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Video Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we can generate an animation of the carrier resource occupation over time\n",
    "movie_name = \"one_nbiot_carrier_movie\"\n",
    "carrier.generate_movie(movie_name = movie_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports for video insertion in the notebook\n",
    "import io\n",
    "import base64\n",
    "from IPython.display import HTML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# code for inserting the video\n",
    "video = io.open(f'./movies/{movie_name}.mp4', 'r+b').read()\n",
    "encoded = base64.b64encode(video)\n",
    "HTML(data='''\n",
    "    <video width=\"360\" height=\"auto\" alt=\"test\" controls><source src=\"data:video/mp4;base64,{0}\" type=\"video/mp4\" /></video>'''\n",
    ".format(encoded.decode('ascii')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Element-by-Element System Creation"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The system can also be created element-by-element using the following classes:\n",
    "* ```MessageSwitch```: a pseudo-broker class to allow message passing among the objects of the system\n",
    "* ```Carrier```: simulates the carriers\n",
    "* ```Channel```: simulates the channel\n",
    "* ```UE_generator```: traffic generation\n",
    "* ```AccessProcedure```: simulates the access procedure in the NPRACH\n",
    "* ```RxProcedure```: simulates data reception for the NPUSCH\n",
    "* ```PerfMonitor```: generates rewards and stores performance metrics\n",
    "* ```NodeB```: simulates the Node B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from node_b import NodeB\n",
    "from population import Population\n",
    "from carrier import Carrier\n",
    "from channel import Channel\n",
    "from access_procedure import AccessProcedure\n",
    "from rx_procedure import RxProcedure\n",
    "from ue_generator import UEGenerator\n",
    "from perf_monitor import PerfMonitor\n",
    "from message_switch import MessageSwitch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the order of creation is important\n",
    "m = MessageSwitch()\n",
    "carrier = Carrier(m, animation = False)\n",
    "channel = Channel(rng, m)\n",
    "ue_generator = UEGenerator(rng, m, M = 2000, ratio = 1)\n",
    "population = Population(rng, m, levels = [0,1,2])\n",
    "access = AccessProcedure(rng,m)\n",
    "receptor = RxProcedure(m)\n",
    "perf_monitor = PerfMonitor(m, reward_criteria = 'average_delay', statistics = True, animation = False)\n",
    "node = NodeB(m, sc_adjustment = True, mcs_automatic = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Agent Creation\n",
    "\n",
    "Agents inherit from the ```DummyAgent``` class which provides the basic atributes:\n",
    "* ```action_items```: list with the action items controlled by the agent\n",
    "* ```s_mask```: list with the state indexes observed by the agent\n",
    "* ```next```: integer pointing to the id of the next agent to take over in the same state\n",
    "* ```states```: list with the Node B states where the agent operates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parameters.py contains predefined s_mask lists\n",
    "from parameters import scheduling_indexes, ce_selection_indexes, nprach_indexes\n",
    "\n",
    "# agents.py defines DummyAgent and a list of actions for NPRACH control\n",
    "from agents import DummyAgent, nprach_actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# agent configurations:\n",
    "\n",
    "agent_0 = {\n",
    "    'id': 0, # index of the agent\n",
    "    'action_items': ['id', 'Imcs', 'Nrep'], # action items controlled by this agent\n",
    "    's_mask': scheduling_indexes, # state indexes observed by this agent\n",
    "    'next': 1, # next agent operating in the same nodeb state\n",
    "    'states': ['Scheduling'] # nodeb state where this agent operates \n",
    "    }\n",
    "\n",
    "agent_1 = {\n",
    "    'id': 1, # carrier, delay and subcarriers\n",
    "    'action_items': ['carrier', 'delay', 'sc'],\n",
    "    's_mask': scheduling_indexes,\n",
    "    'next': -1,\n",
    "    'states': ['Scheduling']\n",
    "    }\n",
    "\n",
    "agent_2 = {\n",
    "    'id': 2, # ce_level selection\n",
    "    'action_items': ['carrier', 'ce_level', 'rar_Imcs', 'delay', 'sc', 'Nrep'],\n",
    "    's_mask': ce_selection_indexes,\n",
    "    'next': -1,\n",
    "    'states': ['RAR_window']\n",
    "}\n",
    "\n",
    "agent_3 = {\n",
    "    'id': 3, # backoff selection\n",
    "    'action_items': ['backoff'],\n",
    "    's_mask': nprach_indexes,\n",
    "    'next': -1,\n",
    "    'states': ['RAR_window_end'],\n",
    "}\n",
    "\n",
    "agent_4 = {\n",
    "    'id': 4, # NPRACH configuration\n",
    "    'action_items': nprach_actions,\n",
    "    's_mask': nprach_indexes,\n",
    "    'next': -1,\n",
    "    'states': ['NPRACH_update']\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# agents are arranged in a list ordered by their id attribute\n",
    "agents = [\n",
    "    DummyAgent(agent_0),\n",
    "    DummyAgent(agent_1),\n",
    "    DummyAgent(agent_2),\n",
    "    DummyAgent(agent_3),\n",
    "    DummyAgent(agent_4)\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Controller Creation\n",
    "\n",
    "The Controller class creates an object that orchestrates the list of agents to operate the system.\n",
    "\n",
    "Two attributes are required:\n",
    "* the controlled system\n",
    "* the list of agents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from controller import Controller"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the controller\n",
    "controller = Controller(node, agents = agents)\n",
    "\n",
    "# reset the environment\n",
    "_ = controller.reset()\n",
    "\n",
    "# run time specified in ms\n",
    "controller.run_until(5000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning with Stable-Baselines Agents\n",
    "\n",
    "One of the agents in the list can be replaced by an external agent provided by stable baselines "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from stable_baselines import A2C\n",
    "from stable_baselines.common.cmd_util import make_vec_env\n",
    "from wrappers import BasicSchedulerWrapper\n",
    "import gym"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reset the environment\n",
    "_ = controller.reset()\n",
    "\n",
    "# set external agent specifying the index\n",
    "controller.set_ext_agent(0)\n",
    "print(' > External agent configured')\n",
    "print(' ')\n",
    "\n",
    "# create the gym environment\n",
    "nbiot_env = gym.make('gym_system:System-v1', system = controller)\n",
    "print(' > gym environment created')\n",
    "print(' ')\n",
    "\n",
    "# wrap the environment\n",
    "nbiot_env = BasicSchedulerWrapper(nbiot_env)\n",
    "print(' > environment wrapped')\n",
    "print(' ')\n",
    "\n",
    "# prepare the agent\n",
    "env = make_vec_env(lambda: nbiot_env, n_envs=1)\n",
    "print(' > vectorised environment created')\n",
    "print(' ')\n",
    "\n",
    "# create the agent \n",
    "model = A2C('MlpPolicy', env, verbose=0, seed = 7)\n",
    "print(' > Model created!')\n",
    "print(' ')\n",
    "\n",
    "# and learn\n",
    "model.learn(total_timesteps = 10_000)\n",
    "print(' > Learning completed!')\n",
    "print(' ')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sequential Learning with Multiple Agents\n",
    "\n",
    "In the following example the learning process is divided into two phases:\n",
    "* First an online learning agent learns how to select the transmission parameters for a selected user\n",
    "* Second an external RL agent from stable baselines learns how to select a user "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the online agent, a random user selection agent, and a predefined agent configuration\n",
    "from agents import OnlineClassifierAgent, RandomUserAgent, agents_conf\n",
    "\n",
    "# import a simple wrapper for the external rl agent selecting the UE\n",
    "from wrappers import BasicWrapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# online classifier (MAMBRL) agent\n",
    "mcs_agent = OnlineClassifierAgent(agents_conf[1], rng)\n",
    "\n",
    "# create agents\n",
    "agents = [\n",
    "    RandomUserAgent(agents_conf[0], rng),\n",
    "    mcs_agent,\n",
    "    DummyAgent(agents_conf[2]),\n",
    "    DummyAgent(agents_conf[3]),\n",
    "    DummyAgent(agents_conf[4]),\n",
    "    DummyAgent(agents_conf[5])\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a new agent must be created \n",
    "controller = Controller(node, agents = agents)\n",
    "_ = controller.reset()\n",
    "\n",
    "# run phase 1\n",
    "controller.run_until(30_000)\n",
    "print(' > Phase 1 completed')\n",
    "print(' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# deactivate mcs learning\n",
    "mcs_agent.deactivate_learning()\n",
    "print(' > Online learning deactivated')\n",
    "print(' ')\n",
    "\n",
    "# reconfigure controller\n",
    "controller.set_ext_agent(0)\n",
    "print(' > Controller reconfigured')\n",
    "print(' ')\n",
    "\n",
    "# move to the next step\n",
    "controller.to_next_step()\n",
    "print(' > Controller advanced to next step')\n",
    "print(' ')\n",
    "\n",
    "# create the gym environment\n",
    "controller.soft_reset = True # to avoid reseting the node b\n",
    "nbiot_env = gym.make('gym_system:System-v1', system = controller)\n",
    "print(' > gym environment created')\n",
    "print(' ')\n",
    "\n",
    "# wrap the environment\n",
    "nbiot_env = BasicWrapper(nbiot_env)\n",
    "print(' > environment wrapped')\n",
    "print(' ')\n",
    "\n",
    "# prepare the agent\n",
    "env = make_vec_env(lambda: nbiot_env, n_envs=1)\n",
    "print(' > vectorised environment created')\n",
    "print(' ')\n",
    "\n",
    "# create the model and learn\n",
    "agent = A2C('MlpPolicy', env, verbose=0, seed = 7)\n",
    "print(' > RL agent created')\n",
    "print(' ')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# learn with the RL agent\n",
    "agent.learn(total_timesteps = 30_000)\n",
    "print(' > Phase 2 completed!')\n",
    "print(' ')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Showing the Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from test_utils import moving_average\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "population.brief_report()\n",
    "node.brief_report()\n",
    "history = moving_average(perf_monitor.delay_history, 100)\n",
    "av_hist = moving_average(perf_monitor.delay_history, 2000)\n",
    "plt.figure()\n",
    "plt.plot(history)\n",
    "plt.plot(av_hist)\n",
    "plt.ylim([0, 1000])\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.7 ('py39')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "b21f740b53d099e370dc73aff7b3ae321aee67cff80624c279b1dd12c3f286e6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
