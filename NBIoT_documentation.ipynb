{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Documentation of the NB_IoT environment\n",
    "\n",
    "This notebook explains how to use the NB_IoT environment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## System Creation with ```create_system```\n",
    "\n",
    "The arguments of this function are:\n",
    "* ```rng```: an random generator created with ```default_rng``` from ```numpy.random```\n",
    "* ```conf```: a dictionary with the system configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import system creation function\n",
    "from system.system_creator import create_system\n",
    "\n",
    "# random number generator object\n",
    "from numpy.random import default_rng"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# simulator configuration\n",
    "conf = {\n",
    "    'animate_carrier': True, # to generate an animation of the carrier occupation\n",
    "    'ratio': 1, # ratio of uniform/beta traffic\n",
    "    'M': 1000, # number of UEs\n",
    "    'buffer_range': [100, 400], # range for the number of bits in the UE buffer\n",
    "    'reward_criteria': 'users', # there are multiple criteria defined in perf_monitor.py\n",
    "    'statistics': True, # to store historical data for statistical evaluation\n",
    "    'animate_stats': False, # to generate an animation of the statsitics over time\n",
    "    'sc_adjustment': True, # to automatically adjust the number of subcarriers\n",
    "    'mcs_automatic': True, # to autimatically select mcs and Nrep\n",
    "    'tx_all_buffer': True\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create random number generator\n",
    "rng = default_rng(seed = 827)\n",
    "\n",
    "# create system\n",
    "node, perf_monitor, population, carrier = create_system(rng, conf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can run a step-by-step simulation of the system as follows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import system.parameters as par\n",
    "\n",
    "def generate_reasonable_action():\n",
    "    action = [0]*23\n",
    "    for name, value in par.control_default_values.items():\n",
    "        index = par.control_items[name]\n",
    "        action[index] = value\n",
    "    return action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate initial action\n",
    "action = generate_reasonable_action()\n",
    "\n",
    "print(action)\n",
    "\n",
    "# reset the system\n",
    "info = node.reset()\n",
    "\n",
    "# simulation loop\n",
    "n = 0\n",
    "while node.time < 800:\n",
    "    n += 1\n",
    "    r, Done, info = node.step(action) \n",
    "    if n % 10 == 0:\n",
    "        print(f'event: {n}, time {node.time}, reward: {r}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Video Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we can generate an animation of the carrier resource occupation process\n",
    "movie_name = \"one_nbiot_carrier_movie\"\n",
    "carrier.generate_movie(movie_name = movie_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports for video insertion in the notebook\n",
    "import io\n",
    "import base64\n",
    "from IPython.display import HTML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# code for inserting the video\n",
    "video = io.open(f'./movies/{movie_name}.mp4', 'r+b').read()\n",
    "encoded = base64.b64encode(video)\n",
    "HTML(data='''\n",
    "    <video width=\"500\" height=\"auto\" alt=\"test\" controls><source src=\"data:video/mp4;base64,{0}\" type=\"video/mp4\" /></video>'''\n",
    ".format(encoded.decode('ascii')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Agent Creation\n",
    "\n",
    "Agents inherit from the ```DummyAgent``` class which provides the basic atributes:\n",
    "* ```action_items```: list with the action items controlled by the agent\n",
    "* ```obs_items```: list with the state items observed by the agent\n",
    "* ```next```: integer pointing to the id of the next agent to take over in the same state\n",
    "* ```states```: list with the Node B states in which the agent operates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from control_agents import DummyAgent\n",
    "\n",
    "# agent configurations:\n",
    "agent_0 = {\n",
    "    'id': 0, # UE Imcs and Nrep selection\n",
    "    'action_items': ['id', 'Imcs', 'Nrep', 'carrier', 'delay', 'sc'], # action items controlled by this agent\n",
    "    # 'obs_items': ['total_ues', 'connection_time', 'loss', 'sinr', 'buffer', 'carrier_state'], # state indexes observed by this agent\n",
    "    'obs_items': [],\n",
    "    'next': -1, # next agent operating in the same nodeb state\n",
    "    'states': ['Scheduling'] # nodeb state where this agent operates \n",
    "    }\n",
    "\n",
    "agent_1 = {\n",
    "    'id': 1, # ce_level selection\n",
    "    'action_items': ['ce_level', 'rar_Imcs', 'Nrep'],\n",
    "    'obs_items': [],\n",
    "    'next': -1,\n",
    "    'states': ['RAR_window']\n",
    "}\n",
    "\n",
    "agent_2 = {\n",
    "    'id': 2, # RA parameters selection\n",
    "    'action_items': ['rar_window', 'mac_timer', 'transmax', 'panchor', 'backoff'],\n",
    "    'obs_items': [],\n",
    "    'next': -1,\n",
    "    'states': ['RAR_window_end'],\n",
    "}\n",
    "\n",
    "agent_3 = {\n",
    "    'id': 3, # NPRACH configuration\n",
    "    'action_items': ['th_C1', 'th_C0', 'sc_C0', 'sc_C1', 'sc_C2', 'period_C0', 'period_C1', 'period_C2'],\n",
    "    'obs_items': ['detection_ratios', 'colision_ratios', 'msg3_detection', 'NPRACH_occupation', 'av_delay', 'distribution'],\n",
    "    'next': -1,\n",
    "    'states': ['NPRACH_update']\n",
    "}\n",
    "\n",
    "# agents are arranged in a list ordered by their id attribute\n",
    "agents = [\n",
    "    DummyAgent(agent_0),\n",
    "    DummyAgent(agent_1),\n",
    "    DummyAgent(agent_2),\n",
    "    DummyAgent(agent_3)\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Controller Creation\n",
    "\n",
    "The Controller class creates an object that orchestrates the list of agents to operate the system.\n",
    "\n",
    "Two attributes are required:\n",
    "* the controlled system\n",
    "* the list of agents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import system creation and controller\n",
    "from controller import Controller\n",
    "\n",
    "ENV_STATISTICS = True\n",
    "ENV_TRACES = True\n",
    "\n",
    "# simulator configuration\n",
    "conf = {\n",
    "    'statistics': ENV_STATISTICS, # to store historical data for statistical evaluation\n",
    "    'traces': ENV_TRACES,\n",
    "    'ratio': 1.0, # ratio of uniform/beta traffic\n",
    "    'M': 1000, # number of UEs\n",
    "    'buffer_range': [100, 600], # range for the number of bits in the UE buffer\n",
    "    'reward_criteria': 'throughput', # users served\n",
    "    }\n",
    "\n",
    "# create random number generator\n",
    "rng = default_rng(seed = 2)\n",
    "\n",
    "# create system\n",
    "node, perf_monitor, _, _ = create_system(rng, conf)\n",
    "\n",
    "# create the controller\n",
    "controller = Controller(node, agents = agents)\n",
    "\n",
    "# reset the environment\n",
    "_ = controller.reset()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning with Stable-Baselines Agents\n",
    "\n",
    "One of the agents in the list can be replaced by an external agent provided by stable baselines\n",
    "\n",
    "For example, agent 3 is in charge of the RSRP thresholds (that determine the coverage of the CE levels) and the NPRACH parameters (n_sc and periodicity) of the three CE levels.\n",
    "\n",
    "The next cells create an RL agent that takes over agent 3 responsibilities, using the Stable-Baselines3 package."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from stable_baselines3 import A2C # DQN, PPO\n",
    "from stable_baselines3.common.env_util import make_vec_env\n",
    "from wrappers import NPRACH_wrapper_traces, DiscreteActions\n",
    "import gymnasium as gym\n",
    "\n",
    "# set external agent specifying the index\n",
    "controller.set_ext_agent(3)\n",
    "print(' > External agent configured')\n",
    "print(' ')\n",
    "\n",
    "# create the gym environment\n",
    "nbiot_env = gym.make('gym_system:System-v1', system = controller)\n",
    "print(' > gym environment created')\n",
    "print(' ')\n",
    "\n",
    "# wrap the environment\n",
    "metrics = ['departures', 'NPRACH_occupation', 'service_times']\n",
    "nbiot_env = NPRACH_wrapper_traces(nbiot_env, metrics, verbose = False, n_report = 1)\n",
    "print(nbiot_env.action_space)\n",
    "\n",
    "## ONLY FOR DQN\n",
    "# nbiot_env = DiscreteActions(nbiot_env) \n",
    "# print(nbiot_env.action_space)\n",
    "print(' > environment wrapped')\n",
    "print(' ')\n",
    "\n",
    "# prepare the agent\n",
    "env = make_vec_env(lambda: nbiot_env, n_envs=1)\n",
    "print(' > vectorised environment created')\n",
    "print(' ')\n",
    "\n",
    "# # check if compatibility of the environment\n",
    "# from stable_baselines3.common import env_checker\n",
    "# env_checker.check_env(nbiot_env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the agent \n",
    "model = A2C('MlpPolicy', env, verbose=0, seed = 21)\n",
    "print(' > Model created!')\n",
    "print(' ')\n",
    "\n",
    "# determine time_steps\n",
    "steps = 5_000\n",
    "\n",
    "# and learn\n",
    "model.learn(total_timesteps = steps)\n",
    "print(' > Learning completed!')\n",
    "print(' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from plot_traces import plot_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_metrics(nbiot_env)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Control with Model Based Agent + RL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we use a MBRL agent as agent 3, i.e. to control RSRP thresholds and NPRACH parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# agents.py defines DummyAgent and a list of actions for NPRACH control\n",
    "from control_agents import DummyAgent\n",
    "# import system creation and controller\n",
    "from system.system_creator import create_system\n",
    "from numpy.random import default_rng\n",
    "from controller import Controller\n",
    "from stable_baselines3 import A2C # SAC #, PPO\n",
    "from stable_baselines3.common.env_util import make_vec_env\n",
    "from wrappers import NPRACH_agent_wrapper\n",
    "from agent_nprach import NPRACH_THAgent\n",
    "import gymnasium as gym\n",
    "\n",
    "# agent configurations:\n",
    "\n",
    "agent_0 = {\n",
    "    'id': 0, # UE Imcs and Nrep selection\n",
    "    'action_items': ['id', 'Imcs', 'Nrep', 'carrier', 'delay', 'sc'], # action items controlled by this agent\n",
    "    # 'obs_items': ['total_ues', 'connection_time', 'loss', 'sinr', 'buffer', 'carrier_state'], # state indexes observed by this agent\n",
    "    'obs_items': [],\n",
    "    'next': -1, # next agent operating in the same nodeb state\n",
    "    'states': ['Scheduling'] # nodeb state where this agent operates \n",
    "    }\n",
    "\n",
    "agent_1 = {\n",
    "    'id': 1, # ce_level selection\n",
    "    'action_items': ['ce_level', 'rar_Imcs', 'Nrep'],\n",
    "    'obs_items': [],\n",
    "    'next': -1,\n",
    "    'states': ['RAR_window']\n",
    "}\n",
    "\n",
    "agent_2 = {\n",
    "    'id': 2, # RA parameters selection\n",
    "    'action_items': ['rar_window', 'mac_timer', 'transmax', 'panchor', 'backoff'],\n",
    "    'obs_items': [],\n",
    "    'next': -1,\n",
    "    'states': ['RAR_window_end'],\n",
    "}\n",
    "\n",
    "agent_3 = {\n",
    "    'id': 3, # NPRACH configuration\n",
    "    'action_items': ['th_C1', 'th_C0', 'sc_C0', 'sc_C1', 'sc_C2', 'period_C0', 'period_C1', 'period_C2'],\n",
    "    'obs_items': ['detection_ratios', 'colision_ratios', 'msg3_detection', 'NPRACH_occupation', 'av_delay', 'distribution'],\n",
    "    'next': -1,\n",
    "    'states': ['NPRACH_update']\n",
    "}\n",
    "\n",
    "# agents are arranged in a list ordered by their id attribute\n",
    "agents = [\n",
    "    DummyAgent(agent_0),\n",
    "    DummyAgent(agent_1),\n",
    "    DummyAgent(agent_2),\n",
    "    DummyAgent(agent_3)\n",
    "]\n",
    "\n",
    "ENV_STATISTICS = False\n",
    "ENV_TRACES = False\n",
    "\n",
    "# simulator configuration\n",
    "conf = {\n",
    "    'statistics': ENV_STATISTICS, # to store historical data for statistical evaluation\n",
    "    'traces': ENV_TRACES,\n",
    "    'ratio': 1.0, # ratio of uniform/beta traffic\n",
    "    'M': 1000, # number of UEs\n",
    "    'buffer_range': [100, 600], # range for the number of bits in the UE buffer\n",
    "    'reward_criteria': 'throughput', # users served\n",
    "    }\n",
    "\n",
    "# create random number generator\n",
    "rng = default_rng(seed = 8) #233\n",
    "\n",
    "# create system\n",
    "node, perf_monitor, _, _ = create_system(rng, conf)\n",
    "\n",
    "# create the controller\n",
    "controller = Controller(node, agents = agents)\n",
    "\n",
    "# reset the environment\n",
    "_ = controller.reset()\n",
    "\n",
    "# set external agent specifying the index\n",
    "controller.set_ext_agent(3)\n",
    "print(' > External agent configured')\n",
    "print(' ')\n",
    "\n",
    "NO_TH = False\n",
    "VERBOSE = False\n",
    "\n",
    "metrics = ['departures', 'NPRACH_occupation', 'service_times', 'beta']\n",
    "# create the intermediate agent\n",
    "agent = NPRACH_THAgent(agent_3, metrics, verbose = VERBOSE, no_th = NO_TH)\n",
    "\n",
    "# create the gym environment\n",
    "nbiot_e = gym.make('gym_system:System-v1', system = controller)\n",
    "\n",
    "# wrap the environment with the intermediate agent\n",
    "observation = [0,1,2,3,4,5,9,10,11,12]\n",
    "nbiot_e = NPRACH_agent_wrapper(nbiot_e, agent, n_actions = 26, obs_items = observation, bounds = [0.3, 2.0])\n",
    "\n",
    "print(nbiot_e.action_space)\n",
    "print(' > environment wrapped')\n",
    "print(' ')\n",
    "\n",
    "# prepare the agent\n",
    "env = make_vec_env(lambda: nbiot_e, n_envs=1)\n",
    "print(' > vectorised environment created')\n",
    "print(' ')\n",
    "\n",
    "# determine time_steps\n",
    "steps = 5_000\n",
    "\n",
    "# # create the agent\n",
    "model = A2C('MlpPolicy', env, verbose = 0, ent_coef = 0.01, seed = 321)\n",
    "print(' > Model created!')\n",
    "print(' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# and learn\n",
    "model.learn(total_timesteps = steps, reset_num_timesteps=False)\n",
    "print(' > Learning completed!')\n",
    "print(' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from plot_traces import plot_metrics\n",
    "\n",
    "plot_metrics(nbiot_e)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py39",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
